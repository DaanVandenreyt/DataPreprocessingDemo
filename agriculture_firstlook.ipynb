{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial intelligence, machine learning, deep learning... Termen die steeds aan populariteit winnen in de wereld van technologie en ver daarbuiten. Er zijn dan ook oneindig veel toepassingen mogelijk en al gekend in verschillende sectoren. Er is dan veel sprake over de modellen en technieken die gebruikt zijn om een zeker probleem aan te pakken met behulp van AI, maar waar helaas veel minder over gesproken wordt is al het werk dat vooraf gaat aan het gebruik van dat model of die techniek: het verwerken en _cleanen_ van de nodige data.\n",
    "\n",
    "Haast iedere toepassing van artificiele intelligentie heeft een massa aan data nodig om accuraat te zijn en zijn werk tot een goed einde te brengen. Voor een demo van een zekere techniek vinden we vaak mooie datasets die meteen te gebruiken zijn, maar helaas is dat in de praktijk zelden het geval. In de ontwikkeling van AI toepassingen gaat er vaak meer tijd verloren aan het verwerken van data dan aan het ontwikkelen van een model (dat is buiten de uren voor het trainen van het model gerekend). Dat deel van een project noemen we _data preprocessing_.\n",
    "\n",
    "In de praktijk is vaak te zien dat de data niet meteen geschikt is en er datapunten ontbreken of er heel wat fouten inzitten. Daarnaast rust AI op wiskundige formules en heel wat data kan niet zomaar in die formules gegoten worden. Denk aan woorden/zinnen, afbeeldingen, geluidsbestanden... Maar ook gewoon numerieke gegevens die niet op dezelfde schaal zitten en dus een foute verhouding weergeven. \n",
    "\n",
    "Met deze blog wil ik graag dat deel dat in ieder AI project terug te vinden is wat meer aandacht geven. Daarom heb ik een dataset gezocht waar nog heel wat werk aan is voor het in een model gebruikt kan worden. Doorheen de blog overloop ik de verschillende die hierbij komen kijken. Uiteraard is dit voor ieder project of iedere dataset anders. Er zijn ontzettend veel technieken gekend voor _data preprocessing_, elk goed voor specifieke toepassingen met bijhorende voor- en nadelen. In deze blog zullen dus niet alle mnogelijkheden gebruikt worden, maar wordt hopelijk wel duidelijk hoeveel er komt kijken bij dit proces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De eerste stap is altijd om de nodige _libraries_ te importeren. Voor deze notebook koos ik voor de _pandas library_ om met de data te werken, voor _matplotlib_ voor visualisaties en voor _numpy_ voor zijn gebruiksgemak, goede datastructuren en een makkelijk gebruik van NaN waardes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T09:34:55.956039Z",
     "start_time": "2020-11-19T09:34:55.328225Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daarna is het tijd om de data in te laden. Omdat de data verspreid is over meerdere files, doe ik dit hier nog niet. Ik voorzie wel al de info om verderop snel en makkelijk de juiste bestanden te kunnen uitlezen. Daarom geef ik het pad naar de juiste map mee en een lijst van de continenten die elk gelinkt kunnen worden aan een bestand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T09:34:55.959164Z",
     "start_time": "2020-11-19T09:34:55.957291Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = './agriculture-crop-production/'\n",
    "continents = ['America', 'Africa', 'Asia', 'Europe', 'Oceania']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De data gaat over landbouw van 1961 tot en met 2014 over de verschillende continenten (zonder Alaska).\n",
    "De huidige opzet van de data is zeer inefficient om snel gegevens op te vragen volgens bepaalde zoektermen. Ter illustratie laad ik alvast het kleinste bestand, dat van Oceanie, in in een _pandas DataFrame_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:01:22.258154Z",
     "start_time": "2020-11-18T12:01:22.063797Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + 'Oceania_reformed.csv', engine='python')\n",
    "\n",
    "# TODO: wat is engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T12:52:27.074858Z",
     "start_time": "2020-11-17T12:52:27.042121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area Code</th>\n",
       "      <th>Area</th>\n",
       "      <th>Item Code</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year</th>\n",
       "      <th>YearF</th>\n",
       "      <th>Area harvested</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Production</th>\n",
       "      <th>Seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1961</td>\n",
       "      <td>F</td>\n",
       "      <td>500.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1962</td>\n",
       "      <td>F</td>\n",
       "      <td>500.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1963</td>\n",
       "      <td>F</td>\n",
       "      <td>500.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1964</td>\n",
       "      <td>F</td>\n",
       "      <td>500.0</td>\n",
       "      <td>22600.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>22395.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>486.0</td>\n",
       "      <td>13992.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1967</td>\n",
       "      <td>F</td>\n",
       "      <td>550.0</td>\n",
       "      <td>21436.0</td>\n",
       "      <td>1179.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1968</td>\n",
       "      <td>F</td>\n",
       "      <td>630.0</td>\n",
       "      <td>17857.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1969</td>\n",
       "      <td>F</td>\n",
       "      <td>700.0</td>\n",
       "      <td>27286.0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>486</td>\n",
       "      <td>Bananas</td>\n",
       "      <td>1970</td>\n",
       "      <td>F</td>\n",
       "      <td>700.0</td>\n",
       "      <td>33829.0</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Area Code            Area  Item Code     Item  Year YearF  Area harvested  \\\n",
       "0          5  American Samoa        486  Bananas  1961     F           500.0   \n",
       "1          5  American Samoa        486  Bananas  1962     F           500.0   \n",
       "2          5  American Samoa        486  Bananas  1963     F           500.0   \n",
       "3          5  American Samoa        486  Bananas  1964     F           500.0   \n",
       "4          5  American Samoa        486  Bananas  1965   NaN           405.0   \n",
       "5          5  American Samoa        486  Bananas  1966   NaN           486.0   \n",
       "6          5  American Samoa        486  Bananas  1967     F           550.0   \n",
       "7          5  American Samoa        486  Bananas  1968     F           630.0   \n",
       "8          5  American Samoa        486  Bananas  1969     F           700.0   \n",
       "9          5  American Samoa        486  Bananas  1970     F           700.0   \n",
       "\n",
       "     Yield  Production  Seed  \n",
       "0  12000.0       600.0   NaN  \n",
       "1  12000.0       600.0   NaN  \n",
       "2  14400.0       720.0   NaN  \n",
       "3  22600.0      1130.0   NaN  \n",
       "4  22395.0       907.0   NaN  \n",
       "5  13992.0       680.0   NaN  \n",
       "6  21436.0      1179.0   NaN  \n",
       "7  17857.0      1125.0   NaN  \n",
       "8  27286.0      1910.0   NaN  \n",
       "9  33829.0      2368.0   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De kolommen hervormen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit eerste deel is geen standaard onderdeel van _data preprocessing_, maar gezien de inefficiënte indeling van de dataset wil ik starten met deze opnieuw in te delen\n",
    "\n",
    "Momenteel zijn er voor ieder jaar twee kolommen, een voor de gegevens en een die aangeeft of de gegevens exact juist zijn of op een andere manier geschat zijn. Daarnaast is er een kolom die aangeeft of een zekere rij data weergeeft van '_production_', '_area harvested_', '_yield_' of '_seed_'. Dit resulteert in een totaal van maar liefst 115 kolommen.\n",
    "\n",
    "Daarom lijkt het me beter en efficiënter om 1 kolom te maken voor 'jaar' en aparte kolommen voor '_production_', '_area harvested_', '_yield_' en '_seed_'. Dit is echter ingewikkelder dan het lijkt omdat we voor iedere rij in deze nieuwe dataset in drie verschillende rijen in de oude dataset moeten gaan zoeken. Maar zo een uitdaging ga ik graag aan en na mijn hoofd er een tijdje op te breken ben ik wel op een oplossing gekomen. Voor het algoritme gebruik ik onderstaande functie.\n",
    "\n",
    "De functie krijgt een lijst van dictionaries mee als parameter. Die dictionaries komen overeen met de rijen uit de ingelezen dataset en met een for-lus wordt over deze rijen geïterreerd. Buiten de for-lus wordt een nieuwe dictionary aangemaakt waar de hervormde data in wordt opgeslagen.\n",
    "\n",
    "Binnen de for-lus wordt opnieuw een lus opgezet om de verschillende jaren te doorlopen. Hierin wordt steeds een specifieke rij van de hervormde data aangemaakt of bijgewerkt. In de nieuwe dataset zal de combinatie van 'Area Code', 'Item Code' en het jaartal uniek zijn voor iedere rij. Deze wordt dan ook gebruikt als _key_ om het onderscheid te maken tussen het aanmaken van een nieuwe rij of een bestaande rij te bewerken.\n",
    "\n",
    "Als de gemaakte _key_ nog niet terug te vinden is, wil dat zeggen dat die rij nog niet bestaat. Deze wordt dan aangemaakt en alle gegevens behalve die van 'Element' en de bijhorende waarde voor dat jaar. Achteraf wordt 'Element' gebruikt als kolom en wordt de bijhorende waarde voor dat jaar gebruikt als waarde voor die kolom. Dat gebeurd ook als de _key_ wel al gevonden was, op die manier wordt die rij aangepast zodat uiteindelijk alle data voor de verschillende mogelijkheden uit 'Element' in dezelfde rij zijn opgeslagen.\n",
    "\n",
    "Enkel de kolommen 'Unit' en 'Element Code' uit de oude set gaan zo verloren, maar de waardes hiervan zijn 1 op 1 gelinkt aan de waardes in 'Element' waardoor die kolommen ook niet meer in de nieuwe dataset zouden passen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T09:45:54.495201Z",
     "start_time": "2020-11-16T09:45:54.487957Z"
    }
   },
   "outputs": [],
   "source": [
    "def reform_dataset(df_dict):\n",
    "    new_dict = {}\n",
    "\n",
    "    for record in df_dict:\n",
    "        for year in range(1961,2015):\n",
    "            sep = '_'\n",
    "            key = sep.join([str(record['Area Code']), str(record['Item Code']), str(year)])\n",
    "\n",
    "            if key not in new_dict.keys():\n",
    "                new_row = {\n",
    "                    'Key': key,\n",
    "                    'Area Code': record['Area Code'],\n",
    "                    'Area': record['Area'],\n",
    "                    'Item Code': record['Item Code'],\n",
    "                    'Item': record['Item'],\n",
    "                    'Year': year,\n",
    "                    'YearF': record['Y'+str(year)+'F']\n",
    "                       }\n",
    "                new_dict[key] = new_row\n",
    "            \n",
    "            column = record['Element']\n",
    "            new_dict[key][column] = record['Y'+str(year)]\n",
    "\n",
    "\n",
    "    return pd.DataFrame.from_dict(new_dict, orient='index').reset_index().drop(columns=['Key', 'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met dit algoritme kan de data naar wens hervormd worden. Daarvoor worden de lijst van continenten en de PATH-variabele gebruikt om de hervorming voor alle bestanden door te voeren.\n",
    "\n",
    "Voor ieder continent wordt dus het bijhorende csv-bestand in een pandas dataframe ingeladen. Daarop wordt dan de vorige functie uitgevoerd zodat de data hervormd worden. \n",
    "\n",
    "Dankzij deze stappen is het aantal kolommen gezakt van 115 naar 10, nog voor het echte 'cleanen' van start is gegaan. Let wel dat er nu uiteraard veel meer rijen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T09:10:13.195442Z",
     "start_time": "2020-11-16T09:09:58.089718Z"
    }
   },
   "outputs": [],
   "source": [
    "for continent in continents:\n",
    "    df = pd.read_csv(PATH + continent + '.csv', engine='python')\n",
    "    df_reformed = reform_dataset(df.to_dict('records'))\n",
    "    df_reformed.to_csv(PATH + continent + '_reformed.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De hervormde versie van Oceanië wordt opnieuw ingeladen om verder te testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T13:34:31.983534Z",
     "start_time": "2020-11-18T13:34:31.778312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Temporary cell for testing with 1 file\n",
    "\n",
    "df = pd.read_csv(PATH + 'Oceania_reformed.csv', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de opmaak van de dataset verbeterd is, is het ook duidelijker welke data er allemaal beschikbaar is en het tijd is voor de volgende stap: _data cleaning_. De stappen in dit deel zijn altijd vergelijkbaar over verschillende projecten, maar de exacte beslissingen die gemaakt worden zijn afhankelijk van zowel de data als het doel van het project. Om dat het nu voornamelijk de bedoeling is om de verschillende stappen te tonen, zijn de keuzes minder van belang. Wel wil ik graag tonen wat de mogelijkheden zijn.\n",
    "\n",
    "### Noisy data\n",
    "Omdat het nu duidelijk is welke data beschikbaar is, is het ook het duidelijker welke data al dan niet waardevol is en dat brengt ons bij het eerste deel van _data cleaning_: _noisy data_.\n",
    "\n",
    "_Noisy data_ kan op twee dingen wijzen. Het kan gaan om individuele datapunten, in dit geval spreken we van _outliers_ die bijvoorbeeld op een foute meting kunnen wijzen. Anderzijds kan ook een _feature_ als _noisy data_ worden gezien. Dat is de optie die hier geldt; niet alle kolommen in het dataframe hebben namelijk een meerwaarde om analyses te kunnen uitvoeren.\n",
    "\n",
    "Zo zijn er de kolommen 'Area' en 'Area Code' waarvan de waardes dezelfde info geven, gelijkaardig is er 'Item ' en 'Item Code'. Voor leesbaarheid zou het logisch zijn om voor de namen te gaan, maar als de data dan gebruikt wordt voor wiskundige modellen (wat meestal het geval is) zijn er nog extra stappen nodig voordat dat model met die kolommen om kan. Daarom koos ik er in dit geval voor om de codes te houden in de dataset en de combinatie van naam en code in een apart dataframe op te slaan en weg te schrijven. Op die manier kunnen de namen in 'Area' en 'Item' toch nog worden teruggevonden.\n",
    "\n",
    "Daarnaast zijn ook de kolommen 'Yield' en 'YearF' overbodig. 'Yield' is berekend uit 'Production' en 'Area harvested', wat wil weggen dat deze waarden kunnen worden berekend. In dat geval wordt die kolom niet bijgehouden. De andere kolom, 'YearF', geeft de accuraatheid van de gegevens voor die rij aan. Omdat we zelf geen betere schatting gaan kunnen maken dan de makers van deze dataset kunnen we ook deze dataset eigenlijk laten vallen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Noisy data__\n",
    "\n",
    "- As a feature\n",
    "    - Kijken naar meerwaarde van de kolommen in het verhaal\n",
    "    - Code vs naam\n",
    "    - Yield, YearF, Seed?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een tweede aspect van _data cleaning_ is _missing values_. Dit is een probleem dat zich in veel projecten voordoet en waar helaas niet echt een goede aanpak voor is. _Missing values_ zijn terug te vinden als _null_ of NaN-waardes en de beste keuze om hiermee om te gaan is afhankelijk van de hoeveelheid data en de hoeveelheid _missing values_.\n",
    "\n",
    "Als er veel data beschikbaar is kan het een optie zijn om per rij te gaan kijken hoeveel waardes ontbreken. De kolommen \n",
    "\n",
    "__Missing Values__\n",
    "\n",
    "    - Te veel missing values\n",
    "        - rij verwijderen\n",
    "    - Invullen met vervangende waarde\n",
    "        - waarde is afhankelijk van het probleem\n",
    "        - VB kolom 'Seed': NaN wijst erop dat er geen seeds zijn voor dit item, dus vervangen door 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T09:51:51.035876Z",
     "start_time": "2020-11-19T09:51:46.654949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2) (45, 2)\n",
      "(168, 2) (168, 2)\n",
      "(104, 2) (59, 2)\n",
      "(175, 2) (164, 2)\n",
      "(155, 2) (51, 2)\n",
      "(182, 2) (172, 2)\n",
      "(201, 2) (46, 2)\n",
      "(182, 2) (145, 2)\n",
      "(223, 2) (22, 2)\n",
      "(182, 2) (133, 2)\n"
     ]
    }
   ],
   "source": [
    "df_areas = pd.DataFrame(columns=['Area', 'Area Code'])\n",
    "df_items = pd.DataFrame(columns=['Item', 'Item Code'])\n",
    "\n",
    "for continent in continents:\n",
    "    df = pd.read_csv(PATH + continent + '_reformed.csv', engine='python')\n",
    "    \n",
    "    areas = df[['Area', 'Area Code']].drop_duplicates()\n",
    "    df_areas = pd.concat([df_areas, areas])\n",
    "    items = df[['Item', 'Item Code']].drop_duplicates()\n",
    "    df_items = pd.concat([df_items, items]).drop_duplicates()\n",
    "    \n",
    "    df.drop(columns=['Yield', 'YearF', 'Item', 'Area'], inplace=True)\n",
    "    \n",
    "    df['Seed'].fillna(0, inplace=True)\n",
    "    df.dropna(subset=['Area harvested', 'Production'], how='all', inplace=True)\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    df.to_csv(PATH + continent + '_cleaned.csv', index=False, header=True)\n",
    "    \n",
    "df_areas.to_csv(PATH + 'areas.csv', index=False, header=True)\n",
    "df_items.to_csv(PATH + 'items.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:31:49.338376Z",
     "start_time": "2020-11-23T12:31:49.336101Z"
    }
   },
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de data is opgeschoond, is de volgende stap vaak feature scaling. Het is van ontzettend belang voor bepaalde modellen. Meerbepaald van modellen die de afstanden tussen punten gaan berekenen in hun werking. Als niet alle variabelen in de data op dezelfde schaal zitten, kan dit die afstand namelijk gaan beïnvloeden. De bedoeling van _feature scaling_ is daarom het herschalen van de variabelen zodat ze allemaal op dezelfde schaal zitten.\n",
    "\n",
    "### Waarom herschalen?\n",
    "\n",
    "Als het gekozen model gebruik maakt van de afstanden tussen datapunten, heeft het herschalen van de variabelen een rechtstreekse invloed op de kwaliteit van het model. Neem als voorbeeld het aantal wielen van een voertuig en het gewicht van een voertuig in kilogram. Dit is een extreem voorbeeld, maar het is duidelijk dat deze variabelen niet op dezelfde schaal zitten. Het gewicht zal haast altijd veel hoger liggen en daarnaast zijn de waarden voor het gewicht veel breder verspreid. Bij de berekening van de afstanden heeft het gewicht dus veel meer invloed op het resultaat, waardoor een model er van uit gaat dat deze variabele van meer belang is. Door de variabelen te herschalen, zijn ze van gelijk belang voor het model.\n",
    "\n",
    "### Wat doet het?\n",
    "\n",
    "Er valt ontzettend veel te vertellen over _feature scaling_ en de verschillende technieken ervoor, om de lengte van dit artikel te bedrukken beperk ik me hier tot de uitleg van de twee belangrijkste technieken: normalisatie en standaardisatie. \n",
    "\n",
    "Bij normalisatie worden alle gegevens herschaalt naar waardes tussen twee getallen, meestal 0 en 1. Bij standaardisatie worden alle gegevens herschaalt zodat iedere variabele een gemiddelde waarde 0 heeft en standaardafwijking 1. Het is belangrijk om deze uit elkaar te kunnen houden, want vaak worden de twee termen door elkaar gebruikt.\n",
    "\n",
    "De meest bekende normalisatie techniek is Min-Max normalisatie. Hierbij komen alle waardes tussen 0 en 1 te liggen door volgende formule die gebruik maakt van de minimale en maximale waarde van een variabele:\n",
    "\n",
    "<img src=\"min-max-normalisation.jpg\" alt=\"drawing\" style=\"width:200px;\" />\n",
    "\n",
    "Voor standaardisatie is de Z-score de meest bekende. Voor iedere variabele wordt de gemiddelde waarde en de standaardafwijking gebruikt om te herschalen. Volgende formule resulteert dan zoals gezegd in een nieuwe reeks met gemiddelde 0 en standaardafwijing 1:\n",
    "\n",
    "<img src=\"zscore.png\" alt=\"drawing\" style=\"width:200px;\"/>\n",
    "\n",
    "Welke de beste keuze van de twee is is volledig afhankelijk van het project en het gekozen model. Het is dus niet zo dat de een beter werkt dan de ander. De beste optie is wellicht om voor beide opties de resultaten te vergelijken en zo een keuze te maken.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdere mogelijkheden\n",
    "\n",
    "Ook nadat de data is opgeschoond en alle variabelen op eenzelfde schaal zitten, zijn er nog mogelijkheden om de data te verbeteren. Die mogelijkheden zijn zeer sterk afhankelijk van de data zelf, het project en het model. Omdat dit hier dus niet concreet genoeg is, worden andere nogelijkheden niet verder toegepast binnen deze dataset.\n",
    "\n",
    "Volgende technieken kunnen interessant zijn voor een eigen project:\n",
    "\n",
    "- __Clustering__: Rijen kunnen worden samengevoegd o.b.v. de vraag \n",
    "    - VB: Welke regio produceert het meeste producten? Groeperen op Area Code en gemiddelde van iedere regio\n",
    "- __Under- en oversampling__: Bij classificatie. Soms zijn niet alle klassen even sterk vertegenwoordigd in de dataset.\n",
    "    - VB: Onderscheid maken tussen katten en honden, wanneer de dataset 500 afbeeldingen van katten heeft, maar slechts 100 afbeeldingen van honden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals in dit artikel dus overlopen is, komt er heel wat voorbereidend werk kijken bij een dataproject. Er kruipt in de praktijk naast het opstellen van deze stappen ook heel wat tijd in het leren kennen van de data en in het denkproces om tot de juiste stappen te komen. Vaak kruipt er zelfs meer tijd in dat voorbereidende werk dan in bijvoorbeeld het opstellen en trainen van het machine learning model, omdat goede data essentieel is voor zo een model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
